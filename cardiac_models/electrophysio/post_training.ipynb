{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str = \"GINO\"  # or \"GNN\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from data_loading import load_dataset\n",
    "import torch\n",
    "from deepcardio.losses import LpLoss\n",
    "from predict import ModelInference\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = r\"/mnt/research/compbiolab/Ehsan/DeepCardioSim/cardiac_models/electrophysio/data_processed\"\n",
    "\n",
    "if model_str == \"GINO\":\n",
    "    from GINO.model import initialize_GINO_model\n",
    "    model = initialize_GINO_model(n_fno_modes=16)\n",
    "    folder_path=DATA_DIR + '/data_GINO.pt'\n",
    "    from deepcardio.meshdata import BipartiteData\n",
    "    dataset_format=BipartiteData\n",
    "    save_dir = \"./GINO/ckpt/\"\n",
    "    dataprocessor_path = \"./GINO/data_processor.pt\"\n",
    "    from GINO.gino_data_handling import single_case_handling\n",
    "\n",
    "elif model_str == \"GNN\":\n",
    "    from GNN.model import initialize_GNN_model\n",
    "    model = initialize_GNN_model(size_hidden_layers=16)\n",
    "    folder_path=DATA_DIR + '/data_GNN.pt'\n",
    "    from torch_geometric.data import Data\n",
    "    dataset_format=Data\n",
    "    save_dir = \"./GNN/ckpt/ckpt_64/\"\n",
    "    dataprocessor_path = \"./GNN/data_processor.pt\"\n",
    "    from GNN.gnn_data_handling import single_case_handling\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Only 'GINO' or 'GNN' can be passed.\")\n",
    "\n",
    "dltrain, dltest, data_processor = load_dataset(\n",
    "    model=model_str,\n",
    "    folder_path=folder_path,\n",
    "    train_batch_sizes=[1], test_batch_sizes=[1, 1],\n",
    "    use_distributed=False, dataset_format=dataset_format,\n",
    "    dataprocessor_dir='./')\n",
    "\n",
    "model_inference = ModelInference(\n",
    "    model=model,\n",
    "    model_checkpoint_path=save_dir + 'best_model_snapshot_dict.pt',\n",
    "    dataprocessor_path=dataprocessor_path,\n",
    "    single_case_handling=single_case_handling)\n",
    "\n",
    "\n",
    "l2loss = LpLoss(d=2, p=2, reductions='mean')\n",
    "\n",
    "train_loss = l2loss\n",
    "eval_losses={'l2': l2loss}\n",
    "\n",
    "print(f\"EPOCH: {model_inference.current_epoch}, LOSS: {model_inference.best_loss}\")\n",
    "model_inference.data_processor.training, model_inference.model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcardio.neuralop_core.utils import count_model_params\n",
    "count_model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "with open(Path(save_dir).joinpath('metrics_dict.json').as_posix(), 'r') as f:\n",
    "    list_epoch_metrics = json.load(f)\n",
    "\n",
    "epochs = []\n",
    "training_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for metrics_data in list_epoch_metrics:\n",
    "    epochs.append(metrics_data['epoch'])\n",
    "    training_losses.append(metrics_data['train_err'])\n",
    "    test_losses.append(metrics_data['0_l2'])\n",
    "    \n",
    "plt.plot(epochs, training_losses, label=\"Training\")\n",
    "plt.plot(epochs, test_losses, label=\"Validation\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['case_id', 'loss'])\n",
    "for i, sample in enumerate(dltrain[0]):\n",
    "    # if sample['n_pacings'] == 1:\n",
    "    #     continue\n",
    "    model_inference.predict(sample)\n",
    "    output = model_inference.output.to('cuda')\n",
    "    train_loss = l2loss(output, **sample).item()\n",
    "    torch.cuda.empty_cache()\n",
    "    df.loc[i] = [int(sample['label'][0]), train_loss]\n",
    "\n",
    "df.to_csv(f'train_losses.csv', index='case_id')\n",
    "\n",
    "plt.plot(df['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor(df['loss']).mean())\n",
    "torch.topk(torch.tensor(df['loss']), k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loss'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = dltrain[0].dataset[6283]\n",
    "sample['label'], sample['n_pacings'], sample['y'].max(), l2loss(model_inference.predict(sample), **sample).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshfile = '/mnt/research/compbiolab/Ehsan/DeepCardioSim/cardiac_models/electrophysio/data/mesh/case'\n",
    "xdmffile = './GINO/results/xdmf/case'\n",
    "\n",
    "model_inference.predict('/mnt/research/compbiolab/Ehsan/DeepCardioSim/cardiac_models/electrophysio/data/mesh/case6283.vtk')\n",
    "model_inference.case_ID = model_inference.case_ID[:-4]\n",
    "model_inference.write_xdmf(\n",
    "    mesh_directory=meshfile,\n",
    "    xdmf_directory=xdmffile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses = []\n",
    "for sample in dltest[0]:\n",
    "    model_inference.predict(sample)\n",
    "    output = model_inference.output.to('cuda')\n",
    "    test_loss = l2loss(output, **sample).item()\n",
    "    torch.cuda.empty_cache()\n",
    "    val_losses.append(test_loss)\n",
    "plt.plot(val_losses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor(val_losses).mean())\n",
    "torch.topk(torch.tensor(val_losses), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = np.zeros((len(dltest[1]), 9))  # d_iso, n_pacings, 0.0, 0.01, 0.02, 0.05, 0.1, 0.2, refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.00\n",
    "for i, sample in enumerate(dltest[1]):\n",
    "    sample['input_geom'] += torch.randn_like(sample['input_geom']) * (noise * sample['input_geom'].std())\n",
    "    f0 = sample['a'][:, :, 2:5]\n",
    "    sample['a'][:, :, 2:5] += torch.randn_like(f0) * (noise * f0.std())\n",
    "    output = model_inference.predict(sample)\n",
    "    test_loss = l2loss(output, **sample).item()\n",
    "    d_iso = sample['a'][:, 0, 1].mean().item()\n",
    "    test_losses[i, 0] = d_iso\n",
    "    test_losses[i, 1] = sample['n_pacings']\n",
    "    test_losses[i, 2] = test_loss\n",
    "    torch.cuda.empty_cache()\n",
    "plt.plot(test_losses[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor(test_losses[:, 2]).mean())\n",
    "torch.topk(torch.tensor(test_losses[:, 2]), k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dltest[1].dataset[5644]\n",
    "sample['label'], sample['n_pacings'], sample['y'].max(), l2loss(model_inference.predict(sample), **sample).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inference.write_xdmf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(test_losses[:, 2], 99.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['case_id', 'loss'])\n",
    "for i, loss in enumerate(test_losses[:, 2]):\n",
    "    df.loc[i] = [int(dltest[1].dataset[i]['label']), loss]\n",
    "\n",
    "df.to_csv(f'test_losses.csv', index='case_id')\n",
    "# np.save(\"/mnt/home/naghavis/Documents/Research/DeepCardioSim/cardiac_models/electrophysio/data/posttraining/test_GINO.npy\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "folder_path = '/mnt/research/compbiolab/Ehsan/DeepCardioSim/cardiac_models/electrophysio/data_realLV/HFC/mesh'\n",
    "YHC_losses = []\n",
    "vtk_files = []\n",
    "vtk_files.extend(glob.glob(os.path.join(folder_path, '*.vtk')))\n",
    "\n",
    "for i, file in enumerate(vtk_files):\n",
    "    model_inference.predict(file)\n",
    "    output = model_inference.output.to('cuda')\n",
    "    model_inference.case_ID = model_inference.case_ID[:-4]\n",
    "    interpolated_loss = l2loss(output, **model_inference.sample).item()\n",
    "    torch.cuda.empty_cache()\n",
    "    YHC_losses.append(interpolated_loss)\n",
    "    \n",
    "plt.plot(YHC_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor(YHC_losses).mean())\n",
    "torch.topk(torch.tensor(YHC_losses), k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtk_files[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inference.predict(vtk_files[33])\n",
    "model_inference.case_ID = model_inference.case_ID[:-4]\n",
    "model_inference.write_xdmf(\n",
    "    mesh_directory='/mnt/research/compbiolab/Ehsan/DeepCardioSim/cardiac_models/electrophysio/data_realLV/HFC/mesh/case',\n",
    "    xdmf_directory='./GINO/results/xdmf/case')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inference.predict('/mnt/home/naghavis/Documents/Research/heArt/generate_EP_dataset/data/mesh/case177.vtk')\n",
    "l2loss(model_inference.output, **model_inference.sample).item(), model_inference.case_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inference.case_ID = model_inference.case_ID[:-4]\n",
    "model_inference.write_xdmf(\n",
    "    mesh_directory='/mnt/home/naghavis/Documents/Research/heArt/generate_EP_dataset/data/mesh/case',\n",
    "    xdmf_directory='/mnt/home/naghavis/Documents/Research/DeepCardioSim/real_geom_res/case'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('_refinedinterp.vtk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dltest[1].dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "folder_path = './data/interpolated'\n",
    "interpolated_losses = []\n",
    "vtk_files = []\n",
    "vtk_files.extend(glob.glob(os.path.join(folder_path, '*.vtk')))\n",
    "\n",
    "meshes = []\n",
    "# for i, file in enumerate(vtk_files):\n",
    "    # model_inference.predict(file)\n",
    "    # output = model_inference.output.to('cuda')\n",
    "    # model_inference.case_ID = model_inference.case_ID[:-4]\n",
    "    # interpolated_loss = l2loss(output, **model_inference.sample).item()\n",
    "    # torch.cuda.empty_cache()\n",
    "    # interpolated_losses.append(interpolated_loss)\n",
    "    \n",
    "# plt.plot(interpolated_losses)\n",
    "# plt.show()\n",
    "\n",
    "d_iso_dev_1ploc = []\n",
    "d_iso_dev_2ploc = []\n",
    "for i, file in enumerate(vtk_files):\n",
    "    model_inference.predict(file)\n",
    "    for data in dltest[1]:\n",
    "        if data['label'] == model_inference.sample['label'][:-18]:\n",
    "            n_pacings = data['n_pacings']\n",
    "            break\n",
    "    d_iso = model_inference.sample['a'][:, 0, 1].cpu().numpy().mean()\n",
    "    max_tact = model_inference.sample['y'].max().cpu().numpy()\n",
    "    if n_pacings == 1:\n",
    "        d_iso_dev_1ploc.append(d_iso)\n",
    "    elif n_pacings == 2:\n",
    "        d_iso_dev_2ploc.append(d_iso)\n",
    "np.save('./data_posttraining/d_iso_interp_1ploc.npy', d_iso_dev_1ploc)\n",
    "np.save('./data_posttraining/d_iso_interp_2ploc.npy', d_iso_dev_2ploc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor(interpolated_losses).mean())\n",
    "torch.topk(torch.tensor(interpolated_losses), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./data_posttraining/interp_GINO.npy\", interpolated_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(test_losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(torch.tensor(test_losses), k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.DataFrame(columns=['Loss', 'd_iso', 'n_pacings', 'max_tact'])\n",
    "d_iso_dev_1ploc = []\n",
    "d_iso_dev_2ploc = []\n",
    "for sample in dltrain[0]:\n",
    "    # loss = l2loss(model_inference.predict(sample), **sample).item()\n",
    "    n_pacings = sample['n_pacings'].cpu().numpy()\n",
    "    d_iso = sample['a'][:, 0, 1].cpu().numpy().mean()\n",
    "    max_tact = sample['y'].max().cpu().numpy()\n",
    "    if n_pacings == 1:\n",
    "        d_iso_dev_1ploc.append(d_iso)\n",
    "    elif n_pacings == 2:\n",
    "        d_iso_dev_2ploc.append(d_iso)\n",
    "    # df.loc[len(df)] = [loss, d_iso, n_pacings, max_tact]\n",
    "np.save('./data_posttraining/d_iso_train_1ploc.npy', d_iso_dev_1ploc)\n",
    "np.save('./data_posttraining/d_iso_train_2ploc.npy', d_iso_dev_2ploc)\n",
    "# df.to_pickle('./data_posttraining/test_GINO_withoutnoise.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "for sample in dltest[1]:\n",
    "    i += 1\n",
    "    if sample['n_pacings'] == 2 or sample['y'].max() < 120 or sample['y'].max() > 150:\n",
    "        continue\n",
    "    print(i, sample['label'], sample['n_pacings'], sample['y'].max(), l2loss(model_inference.predict(sample), **sample).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse problem\n",
    "import meshio\n",
    "def optimization_function_wrapper(x):\n",
    "    \"\"\"\n",
    "    This wraps the function for the inverse modeling\n",
    "    optimization problem.\n",
    "\n",
    "    Input:\n",
    "    - x: a numpy array represents [D_iso, x_l, x_c]\n",
    "\n",
    "    Output:\n",
    "    - loss: The loss of the optimization problem, returned as a\n",
    "    scalar numpy value\n",
    "    \"\"\"\n",
    "    ploc_pmtr = [x[1], x[2], 1]\n",
    "    geometry = meshio.read('./data/geometry' + sample['label'] + '.vtk')\n",
    "    pmtr_coord = np.concatenate((\n",
    "        geometry.point_data['x_l'],\n",
    "        geometry.point_data['x_c'],\n",
    "        geometry.point_data['x_t']), axis=1)\n",
    "\n",
    "    min_loc = np.argmin(np.linalg.norm(pmtr_coord - np.array(ploc_pmtr), axis=1))\n",
    "    ploc_xyz = geometry.points[min_loc].reshape((1, -1)) * 100\n",
    "\n",
    "\n",
    "    output = model_inference.predict(sample, Diso=x[0], plocs=ploc_xyz)\n",
    "\n",
    "    loss = l2loss(output, sample['y'])\n",
    "    return loss.item()\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "bounds = [(0.1, 2.), (0.15, .8), (-3., 3.)]\n",
    "\n",
    "result = differential_evolution(optimization_function_wrapper,\n",
    "                                        bounds,\n",
    "                                        seed=45,\n",
    "                                        popsize=50,\n",
    "                                        strategy='best1bin',\n",
    "                                        maxiter=100,\n",
    "                                        disp=True)\n",
    "\n",
    "result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "0.12689291957489515\n",
    "[0.3247652436498567, -2.853311347773299]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRT Optimization\n",
    "def optimization_function_wrapper(x):\n",
    "    \"\"\"\n",
    "    This wraps the function for the inverse modeling\n",
    "    optimization problem.\n",
    "\n",
    "    Input:\n",
    "    - x: a numpy array represents [x_l2, x_c2]\n",
    "\n",
    "    Output:\n",
    "    - loss: The loss of the optimization problem, returned as a\n",
    "    scalar numpy value\n",
    "    \"\"\"\n",
    "    plocs_pmtr = [[result.x[1], result.x[2], 1], [x[0], x[1], 1]]\n",
    "    geometry = meshio.read('./data/geometry' + sample['label'] + '.vtk')\n",
    "    pmtr_coord = np.concatenate((\n",
    "        geometry.point_data['x_l'],\n",
    "        geometry.point_data['x_c'],\n",
    "        geometry.point_data['x_t']), axis=1)\n",
    "    \n",
    "    plocs_xyz = []\n",
    "    for ploc_ in plocs_pmtr:\n",
    "        min_loc = np.argmin(np.linalg.norm(pmtr_coord - np.array(ploc_), axis=1))\n",
    "        ploc_xyz = geometry.points[min_loc] * 100\n",
    "        plocs_xyz.append(ploc_xyz)\n",
    "\n",
    "    plocs_xyz = np.array(plocs_xyz)\n",
    "\n",
    "\n",
    "    output = model_inference.predict(sample, Diso=result.x[0], plocs=plocs_xyz)\n",
    "\n",
    "    return output.max().item()\n",
    "\n",
    "bounds = [(0.15, .8), (-3., 3.)]\n",
    "\n",
    "result_CRT = differential_evolution(\n",
    "    optimization_function_wrapper,bounds,\n",
    "    seed=45,\n",
    "    popsize=50,\n",
    "    strategy='best1bin',\n",
    "    maxiter=100,\n",
    "    disp=True)\n",
    "\n",
    "result_CRT.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_CRT.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plocs_xyz = []\n",
    "\n",
    "plocs_pmtr = [[result.x[1], result.x[2], 1], [result_CRT.x[0], result_CRT.x[1], 1]]\n",
    "geometry = meshio.read('./data/geometry' + sample['label'] + '.vtk')\n",
    "pmtr_coord = np.concatenate((\n",
    "    geometry.point_data['x_l'],\n",
    "    geometry.point_data['x_c'],\n",
    "    geometry.point_data['x_t']), axis=1)\n",
    "\n",
    "for ploc_ in plocs_pmtr:\n",
    "    min_loc = np.argmin(np.linalg.norm(pmtr_coord - np.array(ploc_), axis=1))\n",
    "    ploc_xyz = geometry.points[min_loc] * 100\n",
    "    plocs_xyz.append(ploc_xyz)\n",
    "\n",
    "plocs_xyz = np.array(plocs_xyz)\n",
    "\n",
    "plocs_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ploc_pmtr = [result.x[1], result.x[2], 1]\n",
    "geometry = meshio.read('./data/geometry' + sample['label'] + '.vtk')\n",
    "pmtr_coord = np.concatenate((\n",
    "    geometry.point_data['x_l'],\n",
    "    geometry.point_data['x_c'],\n",
    "    geometry.point_data['x_t']), axis=1)\n",
    "\n",
    "min_loc = np.argmin(np.linalg.norm(pmtr_coord - np.array(ploc_pmtr), axis=1))\n",
    "ploc_xyz = geometry.points[min_loc].reshape((1, -1)) * 100\n",
    "ploc_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_inference.predict(sample, Diso = result.x[0], plocs=plocs_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for sample in dltest[1]:\n",
    "    if sample['label'] == ['35865']:\n",
    "        print(i)\n",
    "        break\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2701: 9709\n",
    "# 3260: 36214\n",
    "# 22: 26514 0.0108\n",
    "# 37: 18362 0.0079\n",
    "\n",
    "# 4170: 7963 0.0102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dltest[1].dataset[90]\n",
    "sample['label'], sample['n_pacings'], sample['y'].max(), l2loss(model_inference.predict(sample), **sample).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshio\n",
    "def set_rand_pacingsites(sample, rand):\n",
    "    np.random.seed(rand)\n",
    "    x_l = np.random.uniform(0.15, 0.8)\n",
    "    x_c = np.random.uniform(-3, 3)\n",
    "    x_c_2 = np.random.uniform(-3, 3)\n",
    "    plocs_pmtr = [[x_l, x_c, 1], [x_l, x_c_2, 1]]\n",
    "    geometry = meshio.read('../data/geometry_case' + sample['label'] + '.vtk')\n",
    "    pmtr_coord = np.concatenate((\n",
    "        geometry.point_data['x_l'],\n",
    "        geometry.point_data['x_c'],\n",
    "        geometry.point_data['x_t']), axis=1)\n",
    "\n",
    "    plocs_xyz = []\n",
    "    for ploc_ in plocs_pmtr:\n",
    "        min_loc = np.argmin(np.linalg.norm(pmtr_coord - np.array(ploc_), axis=1))\n",
    "        ploc_xyz = geometry.points[min_loc] * 100\n",
    "        plocs_xyz.append(ploc_xyz)\n",
    "\n",
    "    plocs_xyz = np.array(plocs_xyz)\n",
    "\n",
    "    new_sample = model_inference.set_pacingsite(sample, plocs_xyz)\n",
    "    del new_sample['y']\n",
    "    return new_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_act_min = 200\n",
    "t_act_max = 10\n",
    "for i in range(1000):\n",
    "    new_sample = set_rand_pacingsites(sample, 1000 + i)\n",
    "    output = model_inference.predict(new_sample)\n",
    "    if output.max() > t_act_max:\n",
    "        if output.max() > 131:\n",
    "            continue\n",
    "        t_act_max = output.max()\n",
    "        model_inference.write_xdmf(\n",
    "            inp=sample,\n",
    "            mesh_directory='./data/mesh/case',\n",
    "            xdmf_directory='./GINO/results/xdmf/maxcase')\n",
    "    if output.max() < t_act_min:\n",
    "        t_act_min = output.max()\n",
    "        model_inference.write_xdmf(\n",
    "            inp=sample,\n",
    "            mesh_directory='./data/mesh/case',\n",
    "            xdmf_directory='./GINO/results/xdmf/mincase')\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
