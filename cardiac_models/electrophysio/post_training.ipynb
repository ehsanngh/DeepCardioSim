{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str = \"GNN\"  # or \"GNN\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from data_loading import load_dataset\n",
    "import torch\n",
    "from deepcardio.losses import LpLoss\n",
    "from predict import ModelInference\n",
    "import numpy as np\n",
    "from raw_data_handling import single_case_handling\n",
    "\n",
    "data_path = \"/mnt/research/compbiolab/Ehsan/DeepCardioSim/cardiac_models/electrophysio/data_processed\"\n",
    "save_dir = f'./{model_str}/ckpt/'\n",
    "\n",
    "if model_str == \"GINO\":\n",
    "    from GINO.model import initialize_GINO_model\n",
    "    model = initialize_GINO_model(n_fno_modes=16)\n",
    "    \n",
    "elif model_str == \"GNN\":\n",
    "    from GNN.model import initialize_GNN_model\n",
    "    model = initialize_GNN_model(size_hidden_layers=64)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Only 'GINO' or 'GNN' can be passed.\")\n",
    "\n",
    "dltrain, dltest, data_processor = load_dataset(\n",
    "    model_str=model_str,\n",
    "    folder_path=data_path,\n",
    "    train_batch_sizes=[1], test_batch_sizes=[1, 1],\n",
    "    use_distributed=False,\n",
    "    dataprocessor_dir='./')\n",
    "\n",
    "model_inference = ModelInference(\n",
    "    model=model,\n",
    "    model_checkpoint_path=save_dir + 'best_model_snapshot_dict.pt',\n",
    "    dataprocessor_path=f'./{model_str}/data_processor.pt',\n",
    "    single_case_handling=single_case_handling)\n",
    "\n",
    "\n",
    "l2loss = LpLoss(d=2, p=2, reductions='mean')\n",
    "\n",
    "train_loss = l2loss\n",
    "eval_losses={'l2': l2loss}\n",
    "\n",
    "print(f\"EPOCH: {model_inference.current_epoch}, LOSS: {model_inference.best_loss}\")\n",
    "model_inference.data_processor.training, model_inference.model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcardio.neuralop_core.utils import count_model_params\n",
    "count_model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "with open(Path(save_dir).joinpath('metrics_dict.json').as_posix(), 'r') as f:\n",
    "    list_epoch_metrics = json.load(f)\n",
    "\n",
    "epochs = []\n",
    "training_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for metrics_data in list_epoch_metrics:\n",
    "    epochs.append(metrics_data['epoch'])\n",
    "    training_losses.append(metrics_data['train_err'])\n",
    "    test_losses.append(metrics_data['0_l2'])\n",
    "    \n",
    "plt.plot(epochs, training_losses, label=\"Training\")\n",
    "plt.plot(epochs, test_losses, label=\"Validation\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['case_id', 'loss'])\n",
    "for i, sample in enumerate(dltrain[0]):\n",
    "    model_inference.predict(sample)\n",
    "    output = model_inference.output.to('cuda')\n",
    "    train_loss = l2loss(output, **sample).item()\n",
    "    torch.cuda.empty_cache()\n",
    "    df.loc[i] = [int(sample['label'][0]), train_loss]\n",
    "    print(f\"Case ID: {int(sample['label'][0])}, Loss: {train_loss}\")\n",
    "\n",
    "df.to_csv(f'train_losses.csv', index='case_id')\n",
    "\n",
    "plt.plot(df['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor(df['loss']).mean())\n",
    "torch.topk(torch.tensor(df['loss']), k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dltrain[0].dataset[df['loss'].idxmax()]\n",
    "sample['label'], sample['n_pacings'], sample['y'].max(), l2loss(model_inference.predict(sample), **sample).item()\n",
    "model_inference.write_xdmf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Val cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['case_id', 'loss'])\n",
    "for i, sample in enumerate(dltest[0]):\n",
    "    model_inference.predict(sample)\n",
    "    output = model_inference.output.to('cuda')\n",
    "    val_loss = l2loss(output, **sample).item()\n",
    "    torch.cuda.empty_cache()\n",
    "    df.loc[i] = [int(sample['label'][0]), val_loss]\n",
    "    print(f\"Case ID: {int(sample['label'][0])}, Loss: {val_loss}\")\n",
    "    \n",
    "df.to_csv(f'val_losses.csv', index='case_id')\n",
    "\n",
    "plt.plot(df['loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor(df['loss']).mean())\n",
    "torch.topk(torch.tensor(df['loss']), k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['case_id', 'max_tact', 'loss', 'noise1', 'noise5', 'noise10', 'noise20'])\n",
    "\n",
    "for i, sample in enumerate(dltest[1]):\n",
    "    for noise in [0., 0.01, 0.05, 0.1, 0.2]:\n",
    "        sample['input_geom'] += torch.randn_like(sample['input_geom']) * (noise * sample['input_geom'].std())\n",
    "        f0 = sample['a'][:, 2:5]\n",
    "        sample['a'][:, 2:5] += torch.randn_like(f0) * (noise * f0.std())\n",
    "        output = model_inference.predict(sample)\n",
    "        test_loss = l2loss(output, **sample).item()\n",
    "        torch.cuda.empty_cache()\n",
    "        if noise == 0.:\n",
    "            df.loc[\n",
    "                i, ['case_id', 'max_tact', 'loss']\n",
    "                ] = [\n",
    "                    int(sample['label'][0]),\n",
    "                    sample['y'].max().item(),\n",
    "                    test_loss]\n",
    "            print(f\"Case ID: {int(sample['label'][0])}, Max TACT: {sample['y'].max().item()}, Loss: {test_loss}\")\n",
    "        elif noise == 0.01:\n",
    "            df.loc[i, 'noise1'] = test_loss\n",
    "        elif noise == 0.05:\n",
    "            df.loc[i, 'noise5'] = test_loss\n",
    "        elif noise == 0.1:\n",
    "            df.loc[i, 'noise10'] = test_loss\n",
    "        elif noise == 0.2:\n",
    "            df.loc[i, 'noise20'] = test_loss\n",
    "        torch.cuda.empty_cache()\n",
    "df.to_csv(f'test_losses.csv', index='case_id')\n",
    "plt.plot(df['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor(df['loss']).mean())\n",
    "torch.topk(torch.tensor(df['loss']), k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dltest[1].dataset[5371]\n",
    "print(sample['label'], sample['n_pacings'], sample['y'].max(), l2loss(model_inference.predict(sample), **sample).item())\n",
    "model_inference.write_xdmf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(df['loss'], 99.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-world LVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "cohort = 'YHC' # or 'HFC\n",
    "folder_path = f'/mnt/research/compbiolab/Ehsan/DeepCardioSim/cardiac_models/electrophysio/data_realLV/{cohort}/'\n",
    "YHC_losses = []\n",
    "vtk_files = []\n",
    "vtk_files.extend(glob.glob(os.path.join(folder_path, '*.vtk')))\n",
    "\n",
    "df = pd.DataFrame(columns=['case_id', 'loss'])\n",
    "for i, file in enumerate(vtk_files):\n",
    "    model_inference.predict(file)\n",
    "    output = model_inference.output.to('cuda')\n",
    "    model_inference.case_ID = model_inference.case_ID[:-4]\n",
    "    loss = l2loss(output, **model_inference.sample).item()\n",
    "    torch.cuda.empty_cache()\n",
    "    df.loc[i] = [int(model_inference.case_ID), loss]\n",
    "df.to_csv(f'realLV_{cohort}_losses.csv', index='case_id')\n",
    "plt.plot(df['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor(df['loss']).mean())\n",
    "torch.topk(torch.tensor(df['loss']), k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inference.predict(f'/mnt/research/compbiolab/Ehsan/DeepCardioSim/cardiac_models/electrophysio/data_realLV/{cohort}/case19.vtk')\n",
    "model_inference.case_ID = model_inference.case_ID[:-4]\n",
    "model_inference.write_xdmf(\n",
    "    inp_meshdir=f'/mnt/research/compbiolab/Ehsan/DeepCardioSim/cardiac_models/electrophysio/data_realLV/{cohort}/case19.vtk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RR cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_interpolated_cases import refine_case_using_templates\n",
    "df = pd.DataFrame(columns=['case_id', 'lossRR1', 'lossRR2'])\n",
    "\n",
    "for i, sample in enumerate(dltest[1]):\n",
    "    # if not np.isnan(df.loc[i]['lossRR2']):\n",
    "    #     continue\n",
    "    case_ID = sample['label'][0]\n",
    "    npyfile = f'/mnt/research/compbiolab/Ehsan/DeepCardioSim/cardiac_models/electrophysio/data/npy/case{case_ID}_nplocs1.npy'\n",
    "    if not Path(npyfile).exists():\n",
    "        npyfile = f'/mnt/research/compbiolab/Ehsan/DeepCardioSim/cardiac_models/electrophysio/data/npy/case{case_ID}_nplocs2.npy'\n",
    "        if not Path(npyfile).exists():\n",
    "            raise ValueError('No default case-specific npy file found for setting up the mesh please pass the input mesh directory and try again.')\n",
    "    npydata = np.load(npyfile)\n",
    "    points_coarse = npydata[:, :3]\n",
    "\n",
    "    coarse_pd = {\n",
    "        'ploc_bool': npydata[:, 3:4],\n",
    "        'D_iso': npydata[:, 4:5],\n",
    "        'ef': npydata[:, 5:8],\n",
    "        'activation_time': npydata[:, 8:9]\n",
    "    }\n",
    "\n",
    "    mesh_RR1 = refine_case_using_templates(points_coarse, coarse_pd, level='RR1')\n",
    "    mesh_RR1.save(f'./case{case_ID}_RR1.vtk')\n",
    "    model_inference.predict(inp = f'./case{case_ID}_RR1.vtk')\n",
    "    Path(f'./case{case_ID}_RR1.vtk').unlink()\n",
    "    output = model_inference.output.to('cuda')\n",
    "    model_inference.case_ID = model_inference.case_ID[:-8]\n",
    "    model_inference.sample['label'] = model_inference.case_ID\n",
    "    loss_RR1 = l2loss(output, **model_inference.sample).item()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    mesh_RR2 = refine_case_using_templates(points_coarse, coarse_pd, level='RR2')\n",
    "    mesh_RR2.save(f'./case{case_ID}_RR2.vtk')\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda'):\n",
    "        model_inference.predict(inp = f'./case{case_ID}_RR2.vtk')\n",
    "    \n",
    "    output = model_inference.output.to('cuda')\n",
    "    model_inference.case_ID = model_inference.case_ID[:-8]\n",
    "    model_inference.sample['label'] = model_inference.case_ID\n",
    "    loss_RR2 = l2loss(output, **model_inference.sample).item()\n",
    "    torch.cuda.empty_cache()\n",
    "    Path(f'./case{case_ID}_RR2.vtk').unlink()\n",
    "\n",
    "    df.loc[i] = [\n",
    "        int(model_inference.sample['label']),\n",
    "        loss_RR1,\n",
    "        loss_RR2]\n",
    "    print(f'{i}:, {loss_RR1}, {loss_RR2}')\n",
    "\n",
    "    if (i+1) % 100 == 0:\n",
    "        df.to_csv(f'refined_losses.csv', index='case_id')\n",
    "    \n",
    "df.to_csv(f'refined_losses.csv', index='case_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRT workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse problem\n",
    "import meshio\n",
    "sample = dltest[1].dataset[2185] # case 48245\n",
    "sample = sample.to(model_inference.device)\n",
    "geometry = meshio.read('/mnt/home/naghavis/Documents/Research/DeepCardioSim/deepcardio/LVmean/LV_mean.vtk')\n",
    "npy_data = np.load('/mnt/research/compbiolab/Ehsan/DeepCardioSim/cardiac_models/electrophysio/data/npy/case48245_nplocs1.npy')\n",
    "geometry.points = npy_data[:, :3]\n",
    "pmtr_coord = np.concatenate((\n",
    "        geometry.point_data['x_l'],\n",
    "        geometry.point_data['x_c'],\n",
    "        geometry.point_data['x_t']), axis=1)\n",
    "\n",
    "measured_act = torch.tensor(\n",
    "    np.array(meshio.read('sample_crt.vtk').point_data['t_act'], dtype=np.float64),\n",
    "    dtype=sample['y'].dtype,\n",
    "    device=sample['y'].device)\n",
    "\n",
    "min_act_loc = np.argmin(measured_act.cpu().numpy())\n",
    "\n",
    "x0 = [0.5, pmtr_coord[min_act_loc, 0], pmtr_coord[min_act_loc, 1]]\n",
    "\n",
    "def optimization_function_wrapper(x):\n",
    "    \"\"\"\n",
    "    This wraps the function for the inverse modeling\n",
    "    optimization problem.\n",
    "\n",
    "    Input:\n",
    "    - x: a numpy array represents [D_iso, x_l, x_c]\n",
    "\n",
    "    Output:\n",
    "    - loss: The loss of the optimization problem, returned as a\n",
    "    scalar numpy value\n",
    "    \"\"\"\n",
    "    ploc_pmtr = [x[1], x[2], 1]\n",
    "\n",
    "    min_loc = np.argmin(np.linalg.norm(pmtr_coord - np.array(ploc_pmtr), axis=1))\n",
    "    ploc_xyz = np.array([geometry.points[min_loc]])\n",
    "\n",
    "\n",
    "    output = model_inference.predict(sample, Diso=x[0], plocs=ploc_xyz)\n",
    "\n",
    "    loss = l2loss(output, sample['y'])\n",
    "    return loss.item()\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "bounds = [(0.1, 2.), (0.1, .9), (-3., 3.)]\n",
    "\n",
    "result = differential_evolution(optimization_function_wrapper,\n",
    "                                        bounds,\n",
    "                                        seed=45,\n",
    "                                        popsize=45,\n",
    "                                        strategy='best1bin',\n",
    "                                        x0=x0,\n",
    "                                        maxiter=100,\n",
    "                                        disp=True)\n",
    "\n",
    "print(result.x)\n",
    "\n",
    "print((model_inference.output - sample['y']).abs().max(), (model_inference.output - sample['y']).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization problem\n",
    "def optimization_function_wrapper(x):\n",
    "    \"\"\"\n",
    "    This wraps the function for the inverse modeling\n",
    "    optimization problem.\n",
    "\n",
    "    Input:\n",
    "    - x: a numpy array represents [x_l2, x_c2]\n",
    "\n",
    "    Output:\n",
    "    - loss: The loss of the optimization problem, returned as a\n",
    "    scalar numpy value\n",
    "    \"\"\"\n",
    "    plocs_pmtr = [[result.x[1], result.x[2], 1], [x[0], x[1], 1]]\n",
    "    plocs_xyz = []\n",
    "    for ploc_ in plocs_pmtr:\n",
    "        min_loc = np.argmin(np.linalg.norm(pmtr_coord - np.array(ploc_), axis=1))\n",
    "        ploc_xyz = np.array([geometry.points[min_loc]])\n",
    "        plocs_xyz.append(ploc_xyz)\n",
    "\n",
    "    plocs_xyz = np.array(plocs_xyz)\n",
    "\n",
    "\n",
    "    output = model_inference.predict(sample, Diso=result.x[0], plocs=plocs_xyz)\n",
    "\n",
    "    return output.max().item()\n",
    "\n",
    "bounds = [(0.1, .9), (-3., 3.)]\n",
    "\n",
    "result_CRT = differential_evolution(\n",
    "    optimization_function_wrapper,bounds,\n",
    "    seed=45,\n",
    "    popsize=30,\n",
    "    strategy='best1bin',\n",
    "    maxiter=100,\n",
    "    disp=True)\n",
    "\n",
    "result_CRT.x, result_CRT.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plocs_xyz = []\n",
    "\n",
    "plocs_pmtr = [[result.x[1], result.x[2], 1], [result_CRT.x[0], result_CRT.x[1], 1]]\n",
    "\n",
    "for ploc_ in plocs_pmtr:\n",
    "    min_loc = np.argmin(np.linalg.norm(pmtr_coord - np.array(ploc_), axis=1))\n",
    "    ploc_xyz = np.array([geometry.points[min_loc]])\n",
    "    plocs_xyz.append(ploc_xyz)\n",
    "\n",
    "plocs_xyz = np.array(plocs_xyz)\n",
    "\n",
    "plocs_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_inference.predict(sample, Diso = result.x[0], plocs=plocs_xyz)\n",
    "model_inference.write_xdmf()\n",
    "output.max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepcardiosim (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
